global:
  imagePullSecrets: []

kubefin-agent:
  enabled: true

  replicaCount: 2

  extraArgs: []

  config: |
    CLOUD_PROVIDER: "default"
    CLUSTER_NAME: "kubefin-server"
    CLUSTER_ID: ""
    CUSTOM_CPU_CORE_HOUR_PRICE: ""
    CUSTOM_RAM_GB_HOUR_PRICE: ""
    CPU_CORE_RESERVED: "0.1"
    NODE_RAM_RESERVED: "0.1"
    CPUCORE_RAMGB_PRICE_RATIO: "3"

  resources: {}
  # requests:
  #   cpu: 500m
  #   memory: 1500Mi
  # limits:
  #   cpu: 500m
  #   memory: 1500Mi

  otel:
    enabled: true

    resources: {}
    # requests:
    #   cpu: 500m
    #   memory: 1500Mi
    # limits:
    #   cpu: 500m
    #   memory: 1500Mi

    config: |
      receivers:
        prometheus_simple:
          # The allowed min period is 15s
          collection_interval: 15s
          # Kubefin-agent metrics endpoint
          endpoint: "127.0.0.1:8080"
          metrics_path: "/metrics"
          use_service_account: false
      processors:
        batch:
        memory_limiter:
          # 80% of maximum memory up to 2G
          limit_mib: 1500
          # 25% of limit up to 2G
          spike_limit_mib: 512
          check_interval: 5s
      extensions:
        zpages: {}
        memory_ballast:
          # Memory Ballast size should be max 1/3 to 1/2 of memory.
          size_mib: 683
      exporters:
        prometheusremotewrite:
          # namespace: ""
          # Prometheus data format compatible indicator reporting interface.
          # Example:
          # Insert data into thanos. endpoint: "http://thanos-receive.example.local/api/v1/receive"
          # Insert data into mimir. endpoint: "http://mimir-distributor.example.local/api/v1/push"
          endpoint: 'http://{{ .Release.Name }}-mimir.{{ .Release.Namespace }}:9009/api/v1/push'
      service:
        telemetry:
        extensions: [zpages, memory_ballast]
        pipelines:
          metrics:
            receivers: [prometheus_simple]
            processors: [memory_limiter, batch]
            exporters: [prometheusremotewrite]

  podAnnotations: {}

  # Affinity Configuration, String Type.
  affinity: ""
  # Example:
  # affinity: |
  #   podAntiAffinity:
  #     preferredDuringSchedulingIgnoredDuringExecution:
  #       - podAffinityTerm:
  #           labelSelector:
  #             matchLabels:
  #               {{- include "kubefin-agent.selectorLabels" . | nindent 12 }}
  #           topologyKey: kubernetes.io/hostname
  #         weight: 100

  nodeSelector: {}

  tolerations: []

kubefin-cost-analyzer:
  enabled: true

  costAnalyzer:
    replicaCount: 1
    image:
      repository: kubefin/kubefin-cost-analyzer
      pullPolicy: IfNotPresent
      tag: "v0.1.2"

    # Backend TSDB storage query endpoints
    # Example:
    # Querying data from prometheus. query_backend_endpoint: "http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090"
    # Querying data from thanos. query_backend_endpoint: "http://thanos-query-frontend.example.local:10902"
    # Querying data from mimir. query_backend_endpoint: "http://mimir-query-frontend.example.local:9009/prometheus"
    # Querying data from victoriametrics. query_backend_endpoint: "http://vmselect.example.local:8481/select/<accountID>/prometheus"
    query_backend_endpoint: "http://{{ .Release.Name }}-mimir.{{ .Release.Namespace}}:9009/prometheus"

    extraArgs: []
      # - --v=6

    resources: {}
    #  requests:
    #    cpu: 500m
    #    memory: 1Gi
    #  limits:
    #    cpu: 500m
    #    memory: 1Gi

    podAnnotations: {}

    nodeSelector: {}

    tolerations: []

    # Affinity Configuration, String Type.
    # affinity: ""
    # Example:
    # affinity: |
    #   podAntiAffinity:
    #     preferredDuringSchedulingIgnoredDuringExecution:
    #       - podAffinityTerm:
    #           topologyKey: kubernetes.io/hostname
    #           labelSelector:
    #             matchLabels:
    #               {{- include "kubefin-cost-analyzer.selectorLabels" . | nindent 12 }}
    #         weight: 100


# You can turn off mimir if you use external storage compatible with the Prometheus data format.
mimir:
  enabled: true
  image:
    repository: grafana/mimir
    tag: "2.10.3"
    imagePullPolicy: "IfNotPresent"

  resources: {}
  # requests:
  #   cpu: 500m
  #   memory: 1Gi
  # limits:
  #   cpu: 500m
  #   memory: 1Gi

  service:
    port: 9009

  Probe:
    startupProbe:
      enabled: true
      httpGet:
        path: /ready
        port: http
      initialDelaySeconds: 30
      periodSeconds: 15
      failureThreshold: 100
      timeoutSeconds: 5
    readinessProbe:
      enabled: true
      httpGet:
        path: /ready
        port: http
      initialDelaySeconds: 5
      periodSeconds: 15
      failureThreshold: 2
      successThreshold: 1
      timeoutSeconds: 5

    livenessProbe:
      enabled: true
      httpGet:
        path: /ready
        port: http
      initialDelaySeconds: 5
      periodSeconds: 15
      failureThreshold: 2
      successThreshold: 1
      timeoutSeconds: 5

  config: |
    multitenancy_enabled: false

    blocks_storage:
      backend: filesystem
      # This should be changed to pvc(at least)
      bucket_store:
        sync_dir: /tmp/mimir/tsdb-sync
      filesystem:
        dir: /tmp/mimir/data/tsdb
      tsdb:
        dir: /tmp/mimir/tsdb

    compactor:
      data_dir: /tmp/mimir/compactor
      sharding_ring:
        kvstore:
          store: memberlist

    distributor:
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: memberlist

    ingester:
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: memberlist
        replication_factor: 1

    ruler_storage:
      backend: filesystem
      filesystem:
        dir: /tmp/mimir/rules

    server:
      http_listen_port: {{ .Values.mimir.service.port }}
      log_level: info

    store_gateway:
      sharding_ring:
        replication_factor: 1

    # referring to
    # 1.https://github.com/grafana/loki/issues/5123
    # 2.https://grafana.com/docs/mimir/latest/references/configuration-parameters/
    frontend:
      max_outstanding_per_tenant: 4096
